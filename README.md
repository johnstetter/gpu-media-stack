# GPU AI Stack

This project provides a self-hosted, GPU-accelerated AI stack powered by Docker Compose. It includes local LLM serving (via Ollama), multimodal AI (image generation, translation, transcription), and an extensible chat interface with tools like RAG search and Proxmox integration.

---

## üöÄ Stack Overview

| Service               | Description |
|----------------------|-------------|
| **Ollama**           | Local LLM runtime with GPU support |
| **Open WebUI**       | Chat interface for Ollama with RAG search |
| **SearxNG**          | Privacy-focused metasearch engine for RAG |
| **Stable Diffusion** | Image generation with ComfyUI backend |
| **LibreTranslate**   | GPU-enabled translation microservice |
| **Whishper**         | Audio transcription and translation UI |
| **MongoDB**          | Used by Whishper for storage |
| **Redis**            | Caching for agent-proxmox (LangChain) |
| **agent-proxmox**    | LangChain-based interface to MCP |
| **mcp-proxmox**      | Proxmox API integration backend |

---

## üß± Requirements

- Docker 20.10+ and Docker Compose v2
- GPU with NVIDIA Container Toolkit installed
- External Docker volumes pre-created (see below)
- `.env` file with secrets and configs

---

## üì¶ Setup

1. **Clone the Repo**

```bash
git clone https://gitlab.com/stetter-mcp/gpu-ai-stack.git
cd gpu-ai-stack

2.  **Create `.env`  filel**
Here's a starter template you can tweak:
# Permissions
PUID=1000
PGID=1000

# Redis
REDIS_PASSWORD=yourstrongpassword

# Whisper
WHISHPER_HOST=http://whisper:80

# Mongo
DB_USER=whisper
DB_PASS=whisper

# LangChain
LANGCHAIN_LOG_DIR=/logs

# Translate
LT_LOAD_ONLY=en,fr,es

    Create External Volumes

Make sure the named volumes exist:

docker volume create ai-ollama
docker volume create ai-open-webui
docker volume create ai-searxng
docker volume create ai-stable-diffusion-data
docker volume create ai-stable-diffusion-output
docker volume create ai-mongo-db
docker volume create ai-mongo-configdb
docker volume create ai-mongo-logs
docker volume create ai-libretranslate-data
docker volume create ai-libretranslate-cache
docker volume create ai-whisper-uploads
docker volume create ai-whisper-logs
docker volume create ai-whisper-models
docker volume create ai-redis-data
docker volume create ai-agent-proxmox-logs
docker volume create ai-mcp-proxmox

    Create the External Network

docker network create gpu-ai-stack

    Start the Stack

docker compose up -d

üåê Accessing Services
Service	URL
Open WebUI	http://localhost:8080
Stable Diffusion	http://localhost:7860
Whisper UI	http://localhost:8000
SearxNG	http://localhost:8081
LibreTranslate	http://localhost:5000
MongoDB	mongodb://localhost:27017
Proxmox Agent UI	http://localhost:8501
MCP Proxmox API	http://localhost:8008
üß† Agent and MCP Integration

This stack includes two GitLab-hosted services:

    agent-proxmox: LangChain-based interface for LLM-to-Proxmox tasks

    mcp-proxmox: REST API bridge to Proxmox VE nodes

You can issue LLM chat prompts (e.g., in Open WebUI) that forward Proxmox requests via these services using a custom /proxmox plugin or slash command handler.
üßπ Cleanup

docker compose down
docker volume prune
docker network rm gpu-ai-stack

üõ† Dev Notes

    GPU access is enabled via gpus: "all" ‚Äî ensure your NVIDIA drivers + container toolkit are installed.

    Bind mounts like /etc/timezone and /etc/localtime ensure proper container time sync.

    All models and data are persisted via named external volumes.

‚ú® Credits

    Ollama

    Open WebUI

    SearxNG

    LibreTranslate

    ComfyUI

    Whishper

    LangChain

    Proxmox VE

ü§ñ Maintainer

John Stetter
Self-hosted AI tinker ‚Ä¢ Wheelchair adventurer ‚Ä¢ GPU stack wrangler
GitLab


